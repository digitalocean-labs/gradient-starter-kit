{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f343e43",
   "metadata": {},
   "source": [
    "# DigitalOcean Gradient AI - Quick Reference Guide\n",
    "\n",
    "If you're looking to get started with [DigitalOcean Gradient](https://docs.digitalocean.com/products/gradient-ai-agentic-cloud/) and want to quickly start building, this is a collection of examples, code snippets, and guides to help you hit the ground running.\n",
    "\n",
    "Gradient gives you **serverless inference** ‚Äî one API key that lets you use models from **OpenAI, Anthropic, DeepSeek, Llama, Qwen**, and others ‚Äî all from a single endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "## What You Can Do with Gradient\n",
    "\n",
    "**DigitalOcean Gradient** provides two core AI platform offerings:\n",
    "\n",
    "1. **Inference** ‚Äî  \n",
    "   Run models like GPT-4, Llama 3, DeepSeek, and others directly via a unified API endpoint.  \n",
    "   Ideal for quick text, image, and embedding tasks.\n",
    "\n",
    "2. **Agents** ‚Äî  \n",
    "   Create persistent, context-aware agents connected to your own data ‚Äî docs, URLs, buckets, or files.  \n",
    "   These agents ‚Äúremember‚Äù what you‚Äôve uploaded and respond with that context built in.\n",
    "\n",
    "---\n",
    "\n",
    "## Three Ways to Use Gradient\n",
    "\n",
    "You can interact with either **Inference** or **Agents** in any of these ways:\n",
    "\n",
    "| Method | Description | Example Section |\n",
    "|--------|--------------|----------------|\n",
    "| **cURL / REST API** | Use raw HTTP requests ‚Äî fastest way to test and explore the API. | [Section 0 ‚Üí List Models](#0-getting-started-serverless-inference) / [Section 1 ‚Üí Simple Chat](#1-simple-chat-completion) |\n",
    "| **OpenAI SDK** | Use the official OpenAI client and simply override the `base_url`. Fully compatible with the OpenAI API spec. | [Section 3 ‚Üí Using Python (OpenAI SDK)](#3-using-python-openai-sdk) |\n",
    "| **Gradient SDK** | Use DigitalOcean‚Äôs native Python client (`gradient`) ‚Äî includes both Inference and Agents. | [Section 4 ‚Üí Using Python (Gradient SDK)](#4-using-python-gradient-sdk) |\n",
    "\n",
    "> All three methods are API-compatible ‚Äî you can switch between them with minimal code changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start Summary\n",
    "\n",
    "Here‚Äôs a quick map of what you can do and where to find it in this guide:\n",
    "\n",
    "| Goal | Endpoint | Example Section | SDK Equivalent |\n",
    "|------|-----------|-----------------|----------------|\n",
    "| üîç **List available models** | `GET /v1/models` | [0. Getting Started](#0-getting-started-serverless-inference) | `client.models.list()` |\n",
    "| üí¨ **Chat completion (text)** | `POST /v1/chat/completions` | [1. Simple Chat Completion](#1-simple-chat-completion) | `client.chat.completions.create()` |\n",
    "| üñºÔ∏è **Image generation** | `POST /v1/images/generations` | [2. Image Generation](#2-image-generation) | `client.images.generate()` |\n",
    "| üß† **Query your own data (Agents)** | `POST /api/v1/chat/completions` *(agent-specific endpoint)* | [5. Agents and Knowledge Bases](#5-agents-and-knowledge-bases) | `Gradient(agent_access_key=...).chat.completions.create()` |\n",
    "| ‚öôÔ∏è **List agent models** | `GET /api/v1/models` *(agent-specific endpoint)* | [5. Agents and Knowledge Bases](#5-agents-and-knowledge-bases) | `client.models.list()` |\n",
    "\n",
    "> üí° You can switch seamlessly between **cURL**, the **OpenAI SDK**, or the **Gradient SDK** ‚Äî all follow the same pattern and JSON structure.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Using this Kit\n",
    "\n",
    "This repo contains everything you need to start building with DigitalOcean Gradient AI:\n",
    "\n",
    "* **This README** ‚Äî complete guide with explanations and examples  \n",
    "* **Standalone code files** ‚Äî all curl commands (`.sh`) and Python scripts (`.py`) extracted and numbered by section (e.g., `0_list_models.sh`, `3_openai_simple_chat.py`)  \n",
    "* **Jupyter notebook** ‚Äî an interactive version of this guide (`playground.ipynb`) you can run in VS Code, Jupyter, or Google Colab  \n",
    "\n",
    "Pick whichever format works best for you ‚Äî read the guide, run the scripts directly, or work through the notebook interactively.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Getting Started (Serverless Inference)\n",
    "\n",
    "If you just want to call models directly ‚Äî no setup, no GPUs, no servers ‚Äî this is where you start.\n",
    "\n",
    "Gradient‚Äôs **serverless inference** lets you send prompts to models from OpenAI, Anthropic, DeepSeek, Llama, and more using a **single API key**.\n",
    "\n",
    "You‚Äôll need a **Model Access Key**, which you can create from your DigitalOcean account.\n",
    "\n",
    "Once you have it, try this quick check to make sure it‚Äôs working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82595513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2005  100  2005    0     0   6197      0 --:--:-- --:--:-- --:--:--  6188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"created\":1752255238,\"id\":\"alibaba-qwen3-32b\",\"object\":\"model\",\"owned_by\":\"digitalocean\"},{\"created\":1737056627,\"id\":\"anthropic-claude-3-opus\",\"object\":\"model\",\"owned_by\":\"anthropic\"},{\"created\":1737056613,\"id\":\"anthropic-claude-3.5-haiku\",\"object\":\"model\",\"owned_by\":\"anthropic\"},{\"created\":1726761051,\"id\":\"anthropic-claude-3.5-sonnet\",\"object\":\"model\",\"owned_by\":\"anthropic\"},{\"created\":1743449238,\"id\":\"anthropic-claude-3.7-sonnet\",\"object\":\"model\",\"owned_by\":\"anthropic\"},{\"created\":1753359599,\"id\":\"anthropic-claude-opus-4\",\"object\":\"model\",\"owned_by\":\"anthropic\"},{\"created\":1753359599,\"id\":\"anthropic-claude-sonnet-4\",\"object\":\"model\",\"owned_by\":\"anthropic\"},{\"created\":1738947921,\"id\":\"deepseek-r1-distill-llama-70b\",\"object\":\"model\",\"owned_by\":\"digitalocean\"},{\"created\":1724446781,\"id\":\"llama3-8b-instruct\",\"object\":\"model\",\"owned_by\":\"digitalocean\"},{\"created\":1736801780,\"id\":\"llama3.3-70b-instruct\",\"object\":\"model\",\"owned_by\":\"digitalocean\"},{\"created\":1726761051,\"id\":\"mistral-nemo-instruct-2407\",\"object\":\"model\",\"owned_by\":\"digitalocean\"},{\"created\":1753359599,\"id\":\"openai-gpt-4.1\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1740436938,\"id\":\"openai-gpt-4o\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1740436938,\"id\":\"openai-gpt-4o-mini\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1754595084,\"id\":\"openai-gpt-5\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1757445018,\"id\":\"openai-gpt-5-mini\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1757445022,\"id\":\"openai-gpt-5-nano\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1754595080,\"id\":\"openai-gpt-oss-120b\",\"object\":\"model\",\"owned_by\":\"digitalocean\"},{\"created\":1754595075,\"id\":\"openai-gpt-oss-20b\",\"object\":\"model\",\"owned_by\":\"digitalocean\"},{\"created\":1740436938,\"id\":\"openai-o1\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1751565087,\"id\":\"openai-o3\",\"object\":\"model\",\"owned_by\":\"openai\"},{\"created\":1740436938,\"id\":\"openai-o3-mini\",\"object\":\"model\",\"owned_by\":\"openai\"}],\"object\":\"list\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X GET https://inference.do-ai.run/v1/models \\\n",
    "  -H \"Authorization: Bearer $DIGITAL_OCEAN_MODEL_ACCESS_KEY\" \\\n",
    "  -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a49eb",
   "metadata": {},
   "source": [
    "> If everything‚Äôs set up correctly, you‚Äôll get a list of available models back ‚Äî things like `llama3-8b-instruct`, `gpt-4o-mini`, and `openai-gpt-image-1`.\n",
    "> That‚Äôs your confirmation that the key works and you‚Äôre ready to start making requests.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Simple Chat Completion\n",
    "\n",
    "Start with something small to make sure your setup works.\n",
    "This one uses **llama3-8b-instruct**, a great general-purpose model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5eea679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1045  100   839  100   206    805    197  0:00:01  0:00:01 --:--:--  1003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Octopuses have a truly unique ability: they can lose an arm on purpose and then regrow it. This process is called autotomy, and it allows the octopus to escape from predators while leaving the arm behind to distract them. The detached arm can even mimic the movements of the rest of the octopus for a short time, making it an effective decoy. This remarkable ability has fascinated scientists and marine enthusiasts alike, and it's just one of the many reasons why octopuses are such incredibly intelligent and fascinating creatures!\",\"reasoning_content\":null,\"refusal\":null,\"role\":\"assistant\"}}],\"created\":1759764392,\"id\":\"\",\"model\":\"llama3-8b-instruct\",\"object\":\"chat.completion\",\"usage\":{\"completion_tokens\":108,\"prompt_tokens\":51,\"total_tokens\":159}}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl https://inference.do-ai.run/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer $DIGITAL_OCEAN_MODEL_ACCESS_KEY\" \\\n",
    "  -d '{\n",
    "    \"model\": \"llama3-8b-instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Tell me a fun fact about octopuses.\"}\n",
    "    ]\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff96ea",
   "metadata": {},
   "source": [
    "> You should get back a short text response from the model.\n",
    "> That‚Äôs your ‚Äúit works‚Äù moment. üéâ\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Image Generation\n",
    "\n",
    "Once chat works, you can try generating an image.\n",
    "Same API key, just a different model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b344af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl https://inference.do-ai.run/v1/images/generations \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer $DIGITAL_OCEAN_MODEL_ACCESS_KEY\" \\\n",
    "  -d '{\n",
    "    \"model\": \"openai-gpt-image-1\",\n",
    "    \"prompt\": \"A cute baby sea otter floating on its back in calm blue water\",\n",
    "    \"n\": 1,\n",
    "    \"size\": \"1024x1024\"\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9312e",
   "metadata": {},
   "source": [
    "> That will return JSON with a Base64 image string.\n",
    "\n",
    "If you want to save it as a file, just pipe it through `jq` and `base64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl https://inference.do-ai.run/v1/images/generations \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer $DIGITAL_OCEAN_MODEL_ACCESS_KEY\" \\\n",
    "  -d '{\n",
    "    \"model\": \"openai-gpt-image-1\",\n",
    "    \"prompt\": \"A cute baby sea otter floating on its back in calm blue water\",\n",
    "    \"n\": 1,\n",
    "    \"size\": \"1024x1024\"\n",
    "  }' | jq -r '.data[0].b64_json' | base64 --decode > sea_otter.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa59475",
   "metadata": {},
   "source": [
    "> After a few seconds, you should have a file called **sea_otter.png** sitting in your folder.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Using Python (OpenAI SDK)\n",
    "\n",
    "Once the cURL tests work, it's easier to move to Python.\n",
    "You can use either the **OpenAI SDK** (just point it at the DigitalOcean endpoint) or the **Gradient SDK** (DigitalOcean's native one).\n",
    "\n",
    "---\n",
    "\n",
    "### OpenAI SDK (pointed at DigitalOcean Inference)\n",
    "\n",
    "#### List models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cf16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- alibaba-qwen3-32b\n",
      "- anthropic-claude-3-opus\n",
      "- anthropic-claude-3.5-haiku\n",
      "- anthropic-claude-3.5-sonnet\n",
      "- anthropic-claude-3.7-sonnet\n",
      "- anthropic-claude-opus-4\n",
      "- anthropic-claude-sonnet-4\n",
      "- deepseek-r1-distill-llama-70b\n",
      "- llama3-8b-instruct\n",
      "- llama3.3-70b-instruct\n",
      "- mistral-nemo-instruct-2407\n",
      "- openai-gpt-4.1\n",
      "- openai-gpt-4o\n",
      "- openai-gpt-4o-mini\n",
      "- openai-gpt-5\n",
      "- openai-gpt-5-mini\n",
      "- openai-gpt-5-nano\n",
      "- openai-gpt-oss-120b\n",
      "- openai-gpt-oss-20b\n",
      "- openai-o1\n",
      "- openai-o3\n",
      "- openai-o3-mini\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://inference.do-ai.run/v1/\",\n",
    "    api_key=os.getenv(\"DIGITAL_OCEAN_MODEL_ACCESS_KEY\"),\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(\"-\", m.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72be32",
   "metadata": {},
   "source": [
    "#### Simple chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://inference.do-ai.run/v1/\",\n",
    "    api_key=os.getenv(\"DIGITAL_OCEAN_MODEL_ACCESS_KEY\"),\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"llama3-8b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact about octopuses.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f1cb4",
   "metadata": {},
   "source": [
    "#### Image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os, base64\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://inference.do-ai.run/v1/\",\n",
    "    api_key=os.getenv(\"DIGITAL_OCEAN_MODEL_ACCESS_KEY\"),\n",
    ")\n",
    "\n",
    "result = client.images.generate(\n",
    "    model=\"openai-gpt-image-1\",\n",
    "    prompt=\"A cute baby sea otter, children‚Äôs book drawing style\",\n",
    "    size=\"1024x1024\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "b64 = result.data[0].b64_json\n",
    "with open(\"sea_otter.png\", \"wb\") as f:\n",
    "    f.write(base64.b64decode(b64))\n",
    "\n",
    "print(\"Saved sea_otter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996bada",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Using Python (Gradient SDK)\n",
    "\n",
    "### Gradient SDK (native DigitalOcean client)\n",
    "\n",
    "If you prefer to use DigitalOcean‚Äôs native SDK, the setup is almost the same ‚Äî just a slightly different client.\n",
    "\n",
    "#### Simple chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4edb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient import Gradient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Gradient(model_access_key=os.getenv(\"DIGITAL_OCEAN_MODEL_ACCESS_KEY\"))\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"llama3-8b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact about octopuses.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305631a7",
   "metadata": {},
   "source": [
    "#### Image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f105a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sea_otter.png\n"
     ]
    }
   ],
   "source": [
    "from gradient import Gradient\n",
    "from dotenv import load_dotenv\n",
    "import os, base64\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Gradient(model_access_key=os.getenv(\"DIGITAL_OCEAN_MODEL_ACCESS_KEY\"))\n",
    "\n",
    "result = client.images.generations.create(\n",
    "    model=\"openai-gpt-image-1\",\n",
    "    prompt=\"A cute baby sea otter, children‚Äôs book drawing style\",\n",
    "    size=\"1024x1024\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "b64 = result.data[0].b64_json\n",
    "with open(\"sea_otter.png\", \"wb\") as f:\n",
    "    f.write(base64.b64decode(b64))\n",
    "\n",
    "print(\"Saved sea_otter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd77be1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Handy Differences (at a glance)\n",
    "\n",
    "* **Auth variable**: `DIGITAL_OCEAN_MODEL_ACCESS_KEY`\n",
    "* **OpenAI SDK** ‚Üí add `base_url=\"https://inference.do-ai.run/v1/\"`\n",
    "* **Gradient SDK** ‚Üí use `model_access_key`, no base URL needed\n",
    "* **Image model name** ‚Üí `openai-gpt-image-1` (DigitalOcean‚Äôs version)\n",
    "* Always include `n` and `size` when generating images\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Agents and Knowledge Bases\n",
    "\n",
    "Agents are like custom versions of a model that ‚Äúknow‚Äù your stuff.\n",
    "\n",
    "They sit on top of your uploaded knowledge ‚Äî docs, PDFs, URLs, or connected data sources ‚Äî and respond with that context built in. Think of them as ‚Äúyour model + your knowledge base‚Äù that lives on DigitalOcean.\n",
    "\n",
    "You don‚Äôt have to manage a database or index anything yourself. Gradient handles all of it.\n",
    "\n",
    "---\n",
    "\n",
    "### How it works (conceptually)\n",
    "\n",
    "1. **Create a Knowledge Base**\n",
    "   You can:\n",
    "\n",
    "   * Upload files manually (PDFs, docs, text files)\n",
    "   * Point to a public URL\n",
    "   * Connect a **DigitalOcean Spaces** or **S3 bucket**\n",
    "   * Even link to a **Dropbox folder**\n",
    "\n",
    "2. **Create an Agent**\n",
    "   Once you have a knowledge base, you connect it to an agent.\n",
    "   The agent is what you‚Äôll actually talk to ‚Äî it uses your chosen model (e.g., Llama, GPT-4, etc.) and the knowledge base behind the scenes.\n",
    "\n",
    "3. **Get your Agent Endpoint & Access Key**\n",
    "   Gradient gives you two things:\n",
    "\n",
    "   * An **endpoint URL** for that specific agent\n",
    "   * An **access key** (different from your model access key)\n",
    "\n",
    "You can then query that agent using **cURL**, the **OpenAI SDK**, or the **Gradient SDK** ‚Äî same pattern as before, just with the agent endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1: Chat with an Agent (cURL)\n",
    "\n",
    "Replace `AGENT_ENDPOINT` and `AGENT_ACCESS_KEY` with the values from your Gradient dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2972b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -i \\\n",
    "  -X POST \"$AGENT_ENDPOINT/api/v1/chat/completions\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer $AGENT_ACCESS_KEY\" \\\n",
    "  -d '{\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I make this coffee stronger?\"\n",
    "      }\n",
    "    ],\n",
    "    \"stream\": false,\n",
    "    \"include_functions_info\": false,\n",
    "    \"include_retrieval_info\": false,\n",
    "    \"include_guardrails_info\": false\n",
    "  }'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5a101",
   "metadata": {},
   "source": [
    "> If everything‚Äôs configured correctly, your agent will respond using the knowledge base you connected to it.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Chat with an Agent (OpenAI SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://agents.do-ai.run/v1/\",\n",
    "    api_key=os.getenv(\"DIGITAL_OCEAN_AGENT_ACCESS_KEY\"),\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Summarize the pricing details mentioned in the uploaded docs.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8c3f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example 3: Chat with an Agent (Gradient SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9588b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not resolve authentication method. Expected model_access_key to be set for chat completions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m load_dotenv()\n\u001b[1;32m      7\u001b[0m client \u001b[38;5;241m=\u001b[39m Gradient(agent_access_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIGITAL_OCEAN_AGENT_ACCESS_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3-8b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSummarize the pricing details mentioned in the uploaded docs.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Dropbox/dev_projects/gradient-starter-kit/venv/lib/python3.9/site-packages/gradient/_utils/_utils.py:282\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/dev_projects/gradient-starter-kit/venv/lib/python3.9/site-packages/gradient/resources/chat/completions.py:466\u001b[0m, in \u001b[0;36mCompletionsResource.create\u001b[0;34m(self, messages, model, frequency_penalty, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, presence_penalty, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionCreateResponse \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# This method requires an model_access_key to be set via client argument or environment variable\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel_access_key:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not resolve authentication method. Expected model_access_key to be set for chat completions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m         )\n\u001b[1;32m    469\u001b[0m     headers \u001b[38;5;241m=\u001b[39m extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    470\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel_access_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mheaders,\n\u001b[1;32m    473\u001b[0m     }\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not resolve authentication method. Expected model_access_key to be set for chat completions."
     ]
    }
   ],
   "source": [
    "from gradient import Gradient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Gradient(agent_access_key=os.getenv(\"DIGITAL_OCEAN_AGENT_ACCESS_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Summarize the pricing details mentioned in the uploaded docs.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc08fa",
   "metadata": {},
   "source": [
    "> Notice how the code looks almost identical to the inference examples ‚Äî just a different key and base URL.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Notes\n",
    "\n",
    "* **Inference** ‚Üí uses `https://inference.do-ai.run/v1/`\n",
    "* **Agents** ‚Üí uses `https://agents.do-ai.run/v1/`\n",
    "* You‚Äôll have separate keys for both (`MODEL_ACCESS_KEY` and `AGENT_ACCESS_KEY`)\n",
    "* Both APIs follow the **OpenAI API spec**, so you can use the same SDK for both\n",
    "* The **Gradient SDK** wraps both inference and agent APIs under one interface\n",
    "\n",
    "---\n",
    "\n",
    "### Typical Use Case\n",
    "\n",
    "* Use **Inference** for quick, on-demand tasks (chat, image, embeddings, etc.)\n",
    "* Use **Agents** when you want a persistent model that ‚Äúremembers‚Äù or ‚Äúknows‚Äù something ‚Äî like your documentation, internal policies, or dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Once you've got your agent working, you can integrate it anywhere ‚Äî a chatbot UI, an internal Slack bot, a product support tool, or even your own API layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Using the Jupyter Notebook\n",
    "\n",
    "This repo includes a **playground.ipynb** file ‚Äî an interactive notebook version of this guide.\n",
    "\n",
    "### Setting up your API keys\n",
    "\n",
    "Before running the notebook, you'll need to set up your API keys:\n",
    "\n",
    "**For local use (VS Code, Jupyter):**\n",
    "\n",
    "1. Copy the `.env-example` file to `.env`:\n",
    "   ```bash\n",
    "   cp .env-example .env\n",
    "   ```\n",
    "\n",
    "2. Edit `.env` and add your actual API keys:\n",
    "   - `DIGITAL_OCEAN_MODEL_ACCESS_KEY` ‚Äî for serverless inference\n",
    "   - `AGENT_ENDPOINT` ‚Äî your agent's endpoint URL\n",
    "   - `AGENT_ACCESS_KEY` ‚Äî for agent access\n",
    "\n",
    "**For Google Colab:**\n",
    "\n",
    "Add your keys to the **Secrets** section in Colab (the üîë icon in the sidebar) using these names:\n",
    "- `DIGITAL_OCEAN_MODEL_ACCESS_KEY`\n",
    "- `AGENT_ENDPOINT`\n",
    "- `AGENT_ACCESS_KEY`\n",
    "\n",
    "---\n",
    "\n",
    "You can use the notebook in a few different ways:\n",
    "\n",
    "### Option 1: VS Code (Jupyter Extension)\n",
    "\n",
    "If you're using VS Code, just open `playground.ipynb` and the Jupyter extension will let you run the code cells directly.\n",
    "\n",
    "### Option 2: Jupyter Notebook or JupyterLab\n",
    "\n",
    "If you have Jupyter installed locally, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter notebook playground.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec7915",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93951452",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter lab playground.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a639e4",
   "metadata": {},
   "source": [
    "### Option 3: Google Colab\n",
    "\n",
    "You can also upload `playground.ipynb` to [Google Colab](https://colab.research.google.com/) and run it there ‚Äî no local setup required.\n",
    "\n",
    "---\n",
    "\n",
    "### Regenerating the Notebook\n",
    "\n",
    "If you make changes to this **README.md** file and want to update the notebook, you can use the **jupytext** tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install jupytext\n",
    "jupytext --to notebook README.md -o playground.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618da08c",
   "metadata": {},
   "source": [
    "This will regenerate `playground.ipynb` from the markdown file, keeping everything in sync."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
